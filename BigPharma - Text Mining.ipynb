{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "import re\n",
    "import nltk\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from datetime import datetime\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "companies_mapping = {'AZN_stocks.csv':\"AstraZeneca\", 'RHHBY_stocks.csv':\"Roche\", 'PFE_stocks.csv':\"Pfizer\", \n",
    "                     'NVS_stocks.csv':\"Novartis\",'BAYRY_stocks.csv':\"BayerPharma\", 'MRK_stocks.csv':\"Merck\", \n",
    "                     'GSK_stocks.csv':\"GSK\", 'SNY_stocks.csv':\"Sanofi\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twitter = glob('input/tweets/*.csv')\n",
    "df_tweets = pd.concat(pd.read_csv(file).assign(filename = file) for file in twitter)\n",
    "\n",
    "stock = glob('input/stock/*.csv')\n",
    "df_stock = pd.concat(pd.read_csv(file).assign(filename = file) for file in stock)\n",
    "\n",
    "df_stock.filename = df_stock.filename.str.split(pat =\"\\\\\", expand = True)[1]\n",
    "df_stock['company'] = df_stock.filename.map(companies_mapping)\n",
    "\n",
    "df_covid = pd.read_csv('input/covid_data.csv')\n",
    "df_tweets.reset_index(inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "del df_tweets['filename']\n",
    "del df_tweets['index']\n",
    "\n",
    "del df_stock['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_hashes(row):\n",
    "\n",
    "    return list(set([re.sub('[^\\w\\s]','', word) for word in row.split() if word[0] == '#']))\n",
    "\n",
    "def return_ats(row):\n",
    "    return list(set(  [re.sub('[^\\w\\s]','', word) for word in row.split() if word[0] == '@']  ))\n",
    "\n",
    "def remove_ats(row):\n",
    "    return ' '.join([word for word in row.split() if word[0] != '@'])\n",
    "\n",
    "def remove_stopwords(row):\n",
    "    return ' '.join([word for word in row.split() if word not in stop])\n",
    "\n",
    "def remove_https(row):\n",
    "    return ' '.join([word for word in row.split() if word[0:6] != 'https:' and word[0:5] != 'http:'])\n",
    "\n",
    "\n",
    "def lemmatized(row, lemmatizer):\n",
    "    \n",
    "    tokenized = word_tokenize(row)\n",
    "    \n",
    "    \n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokenized):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word,pos))\n",
    "    return lemmatized_sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower column names\n",
    "df_tweets.columns = map(str.lower, df_tweets.columns)\n",
    "df_stock.columns = map(str.lower, df_stock.columns)\n",
    "\n",
    "df_tweets.rename(columns = {'text':'text_original'}, inplace = True)\n",
    "\n",
    "df_tweets['text_modified'] = df_tweets['text_original'].str.lower() # małe znaki\n",
    "df_tweets['hash'] = df_tweets['text_modified'].apply(return_hashes) # wybieranie unikalnych hashtagów (bez punktuacji)\n",
    "df_tweets['at'] = df_tweets['text_modified'].apply(return_ats) # wybieranie unikalnych odnośników (bez punktuacji)\n",
    "df_tweets['text_modified'] = df_tweets['text_modified'].apply(remove_https)\n",
    "df_tweets['text_modified'] = df_tweets['text_modified'].str.replace('[^\\w\\s]','') # usuwanie punktuacji; można usuwać # ze zdań jeśli usunie się ten znak z regular expression\n",
    "df_tweets['text_modified'] = df_tweets['text_modified'].apply(remove_stopwords)\n",
    "df_tweets['text_lemmatized'] = df_tweets['text_modified'].apply(lemmatized)\n",
    "\n",
    "df_tweets.created_at = pd.to_datetime(df_tweets.created_at)\n",
    "df_tweets['date'] = df_tweets.created_at.dt.date\n",
    "\n",
    "df_stock.date = pd.to_datetime(df_stock.date)\n",
    "df_stock['date'] = df_stock.date.dt.date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zakres datowy dla tweetów   2019-12-11 16:58:23   -   2020-05-06 13:13:41   :  AstraZeneca\n",
      "Zakres datowy dla tweetów   2019-09-26 07:00:01   -   2020-05-06 16:40:37   :  BayerPharma\n",
      "Zakres datowy dla tweetów   2020-02-05 12:26:23   -   2020-05-07 15:41:11   :  GSK\n",
      "Zakres datowy dla tweetów   2020-02-04 14:59:51   -   2020-05-07 18:10:43   :  Merck\n",
      "Zakres datowy dla tweetów   2019-10-17 21:23:19   -   2020-05-07 19:55:40   :  Novartis\n",
      "Zakres datowy dla tweetów   2020-02-17 13:55:00   -   2020-05-07 20:02:01   :  Pfizer\n",
      "Zakres datowy dla tweetów   2020-01-30 14:19:25   -   2020-05-07 14:24:05   :  Roche\n",
      "Zakres datowy dla tweetów   2020-01-29 14:29:27   -   2020-05-05 12:59:16   :  Sanofi\n"
     ]
    }
   ],
   "source": [
    "companies = df_tweets.company.unique()\n",
    "\n",
    "for company in companies:\n",
    "    dat1 = min(df_tweets.loc[df_tweets['company'] == company].created_at)\n",
    "    dat2 = max(df_tweets.loc[df_tweets['company'] == company].created_at)\n",
    "    \n",
    "    print(f'Zakres datowy dla tweetów   {dat1}   -   {dat2}   :  {company}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ze względy na ograniczenie API Twittera dla każdej firmy pobranych zostało 200 tweetów. Zakres czasowy ich występowania różni się dla danych firm. Dlatego ustalony zostaje wspólny okres badania: od 1 lutego do 7 maja. Początego tego okresu można uznać za początek epidemii koronawirusa w Europie i Ameryce Północnej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_date = datetime.strptime('2020-02-02','%Y-%m-%d').date()\n",
    "\n",
    "df_tweets = df_tweets.loc[df_tweets['date'] >= lower_date]\n",
    "df_stock = df_stock.loc[df_stock['date'] >= lower_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponowne sprawdzenie zakresu dat, tym razem z ilością tweetów pozostałą po ograniczeniu zbioru danych dla każdej z firm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zakres datowy dla tweetów   2020-02-03 11:30:17   -   2020-05-06 13:13:41   :  AstraZeneca; il. tweetów: 107\n",
      "Zakres datowy dla tweetów   2020-02-03 16:39:56   -   2020-05-06 16:40:37   :  BayerPharma; il. tweetów: 65\n",
      "Zakres datowy dla tweetów   2020-02-05 12:26:23   -   2020-05-07 15:41:11   :  GSK; il. tweetów: 200\n",
      "Zakres datowy dla tweetów   2020-02-04 14:59:51   -   2020-05-07 18:10:43   :  Merck; il. tweetów: 200\n",
      "Zakres datowy dla tweetów   2020-02-03 13:22:07   -   2020-05-07 19:55:40   :  Novartis; il. tweetów: 106\n",
      "Zakres datowy dla tweetów   2020-02-17 13:55:00   -   2020-05-07 20:02:01   :  Pfizer; il. tweetów: 200\n",
      "Zakres datowy dla tweetów   2020-02-03 09:01:21   -   2020-05-07 14:24:05   :  Roche; il. tweetów: 195\n",
      "Zakres datowy dla tweetów   2020-02-03 10:03:21   -   2020-05-05 12:59:16   :  Sanofi; il. tweetów: 193\n"
     ]
    }
   ],
   "source": [
    "companies = df_tweets.company.unique()\n",
    "\n",
    "for company in companies:\n",
    "    dat1 = min(df_tweets.loc[df_tweets['company'] == company].created_at)\n",
    "    dat2 = max(df_tweets.loc[df_tweets['company'] == company].created_at)\n",
    "    ilosc = len(df_tweets.loc[df_tweets['company'] == company])\n",
    "    print(f'Zakres datowy dla tweetów   {dat1}   -   {dat2}   :  {company}; il. tweetów: {ilosc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać dla BayerPharmy ta ilość jest mniejsza niż poprzednio połowa. Ewentualnością będzie wykluczenie tej firmy z badania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>text_original</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favourite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text_modified</th>\n",
       "      <th>hash</th>\n",
       "      <th>at</th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AstraZeneca</td>\n",
       "      <td>Together with partners across industry, academia and government, we are taking a multipronged approach to helping patients around the world facing #COVID19. https://t.co/uQuHj6BkBN</td>\n",
       "      <td>2020-05-06 13:13:41</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>together partners across industry academia government taking multipronged approach helping patients around world facing covid19</td>\n",
       "      <td>[covid19]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[together, partner, across, industry, academia, government, take, multipronged, approach, help, patient, around, world, facing, covid19]</td>\n",
       "      <td>2020-05-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AstraZeneca</td>\n",
       "      <td>On #GivingTuesdayNow we stand with our partners @Plan_UK @Unicef_UK @ProjectHopeorg @NCDAlliance in their efforts responding to the unique health needs of groups vulnerable to #COVID19, such as those living with NCDs and young people. Get involved: https://t.co/YGRHLGqct6 https://t.co/vePEeAne49</td>\n",
       "      <td>2020-05-05 16:27:03</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>givingtuesdaynow stand partners plan_uk unicef_uk projecthopeorg ncdalliance efforts responding unique health needs groups vulnerable covid19 living ncds young people get involved</td>\n",
       "      <td>[givingtuesdaynow, covid19]</td>\n",
       "      <td>[plan_uk, unicef_uk, projecthopeorg, ncdalliance]</td>\n",
       "      <td>[givingtuesdaynow, stand, partner, plan_uk, unicef_uk, projecthopeorg, ncdalliance, effort, respond, unique, health, need, group, vulnerable, covid19, living, ncds, young, people, get, involve]</td>\n",
       "      <td>2020-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AstraZeneca</td>\n",
       "      <td>We’re #standingtogether4asthma with patients and the respiratory community during these times of uncertainty. Visit @WEF to learn more about what we’re doing to play our part in the fight against #COVID19: #WorldAsthmaDay   \\r\\nhttps://t.co/fWE7ik8rNs https://t.co/Z54pyHBENq</td>\n",
       "      <td>2020-05-05 12:30:15</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>standingtogether4asthma patients respiratory community times uncertainty visit wef learn play part fight covid19 worldasthmaday</td>\n",
       "      <td>[worldasthmaday, standingtogether4asthma, covid19]</td>\n",
       "      <td>[wef]</td>\n",
       "      <td>[standingtogether4asthma, patient, respiratory, community, time, uncertainty, visit, wef, learn, play, part, fight, covid19, worldasthmaday]</td>\n",
       "      <td>2020-05-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       company  \\\n",
       "0  AstraZeneca   \n",
       "1  AstraZeneca   \n",
       "2  AstraZeneca   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                              text_original  \\\n",
       "0  Together with partners across industry, academia and government, we are taking a multipronged approach to helping patients around the world facing #COVID19. https://t.co/uQuHj6BkBN                                                                                                                       \n",
       "1  On #GivingTuesdayNow we stand with our partners @Plan_UK @Unicef_UK @ProjectHopeorg @NCDAlliance in their efforts responding to the unique health needs of groups vulnerable to #COVID19, such as those living with NCDs and young people. Get involved: https://t.co/YGRHLGqct6 https://t.co/vePEeAne49   \n",
       "2  We’re #standingtogether4asthma with patients and the respiratory community during these times of uncertainty. Visit @WEF to learn more about what we’re doing to play our part in the fight against #COVID19: #WorldAsthmaDay   \\r\\nhttps://t.co/fWE7ik8rNs https://t.co/Z54pyHBENq                        \n",
       "\n",
       "           created_at  favourite_count  retweet_count  \\\n",
       "0 2020-05-06 13:13:41  44               8               \n",
       "1 2020-05-05 16:27:03  32               8               \n",
       "2 2020-05-05 12:30:15  19               7               \n",
       "\n",
       "                                                                                                                                                                         text_modified  \\\n",
       "0  together partners across industry academia government taking multipronged approach helping patients around world facing covid19                                                       \n",
       "1  givingtuesdaynow stand partners plan_uk unicef_uk projecthopeorg ncdalliance efforts responding unique health needs groups vulnerable covid19 living ncds young people get involved   \n",
       "2  standingtogether4asthma patients respiratory community times uncertainty visit wef learn play part fight covid19 worldasthmaday                                                       \n",
       "\n",
       "                                                 hash  \\\n",
       "0  [covid19]                                            \n",
       "1  [givingtuesdaynow, covid19]                          \n",
       "2  [worldasthmaday, standingtogether4asthma, covid19]   \n",
       "\n",
       "                                                  at  \\\n",
       "0  []                                                  \n",
       "1  [plan_uk, unicef_uk, projecthopeorg, ncdalliance]   \n",
       "2  [wef]                                               \n",
       "\n",
       "                                                                                                                                                                                     text_lemmatized  \\\n",
       "0  [together, partner, across, industry, academia, government, take, multipronged, approach, help, patient, around, world, facing, covid19]                                                            \n",
       "1  [givingtuesdaynow, stand, partner, plan_uk, unicef_uk, projecthopeorg, ncdalliance, effort, respond, unique, health, need, group, vulnerable, covid19, living, ncds, young, people, get, involve]   \n",
       "2  [standingtogether4asthma, patient, respiratory, community, time, uncertainty, visit, wef, learn, play, part, fight, covid19, worldasthmaday]                                                        \n",
       "\n",
       "         date  \n",
       "0  2020-05-06  \n",
       "1  2020-05-05  \n",
       "2  2020-05-05  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj close</th>\n",
       "      <th>volume</th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2020-02-03</td>\n",
       "      <td>48.619999</td>\n",
       "      <td>48.930000</td>\n",
       "      <td>48.450001</td>\n",
       "      <td>48.509998</td>\n",
       "      <td>47.541225</td>\n",
       "      <td>1990900</td>\n",
       "      <td>AstraZeneca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2020-02-04</td>\n",
       "      <td>48.759998</td>\n",
       "      <td>49.090000</td>\n",
       "      <td>48.720001</td>\n",
       "      <td>48.759998</td>\n",
       "      <td>47.786232</td>\n",
       "      <td>1698800</td>\n",
       "      <td>AstraZeneca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>49.459999</td>\n",
       "      <td>49.849998</td>\n",
       "      <td>49.230000</td>\n",
       "      <td>49.730000</td>\n",
       "      <td>48.736862</td>\n",
       "      <td>2303000</td>\n",
       "      <td>AstraZeneca</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date       open       high        low      close  adj close  \\\n",
       "63  2020-02-03  48.619999  48.930000  48.450001  48.509998  47.541225   \n",
       "64  2020-02-04  48.759998  49.090000  48.720001  48.759998  47.786232   \n",
       "65  2020-02-05  49.459999  49.849998  49.230000  49.730000  48.736862   \n",
       "\n",
       "     volume      company  \n",
       "63  1990900  AstraZeneca  \n",
       "64  1698800  AstraZeneca  \n",
       "65  2303000  AstraZeneca  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stock.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = pd.read_csv('input/sentiment.csv',encoding = \"ISO-8859-1\",header=None)\n",
    "df_sentiment = df_sentiment[[0,5]]\n",
    "df_sentiment.rename(columns={0:'sentiment',5:'text_original'},inplace=True)\n",
    "df_sentiment['sentiment'] = df_sentiment['sentiment'].map({0:'negative',2:'neutral',4:'positive'})\n",
    "# po 100 000 przykładków dla każdego z sentymentów\n",
    "df_sentiment = pd.concat([df_sentiment.loc[df_sentiment['sentiment']=='negative'][0:100000],\n",
    "                        df_sentiment.loc[df_sentiment['sentiment']=='neutral'][0:100000],\n",
    "                        df_sentiment.loc[df_sentiment['sentiment']=='positive'][0:100000] ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mc:\\python 3.7\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3589\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3590\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3591\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3593\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-238783be3a3d>\u001b[0m in \u001b[0;36mlemmatized\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlemmatized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mtokenized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mlemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python 3.7\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     return [\n\u001b[1;32m--> 145\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m     ]\n",
      "\u001b[1;32mc:\\python 3.7\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     return [\n\u001b[1;32m--> 145\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m     ]\n",
      "\u001b[1;32mc:\\python 3.7\\lib\\site-packages\\nltk\\tokenize\\treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_parentheses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_str\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTARTING_QUOTES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_sentiment['text_modified'] = df_sentiment['text_original'].str.lower()\n",
    "df_sentiment['text_modified'] = df_sentiment['text_modified'].apply(remove_https)\n",
    "df_sentiment['text_modified'] = df_sentiment['text_modified'].apply(remove_ats)\n",
    "df_sentiment['text_modified'] = df_sentiment['text_modified'].str.replace('[^\\w\\s]','')\n",
    "df_sentiment['text_modified'] = df_sentiment['text_modified'].apply(remove_stopwords)\n",
    "df_sentiment['text_lemmatized'] = df_sentiment.apply(lambda x: lemmatized(x['text_modified'], lemmatizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l = df_sentiment.loc[df_sentiment['sentiment'] == 'positive']['text_lemmatized'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text_original</th>\n",
       "      <th>text_modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</td>\n",
       "      <td>awww thats bummer shoulda got david carr third day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "      <td>upset cant update facebook texting might cry result school today also blah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "      <td>dived many times ball managed save 50 rest go bounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>whole body feels itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "      <td>behaving im mad cant see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>negative</td>\n",
       "      <td>Need a hug</td>\n",
       "      <td>need hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>negative</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?</td>\n",
       "      <td>hey long time see yes rains bit bit lol im fine thanks hows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>negative</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "      <td>nope didnt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>negative</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "      <td>que muera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>negative</td>\n",
       "      <td>spring break in plain city... it's snowing</td>\n",
       "      <td>spring break plain city snowing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>negative</td>\n",
       "      <td>I just re-pierced my ears</td>\n",
       "      <td>repierced ears</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>negative</td>\n",
       "      <td>@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .</td>\n",
       "      <td>couldnt bear watch thought ua loss embarrassing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>negative</td>\n",
       "      <td>@octolinz16 It it counts, idk why I did either. you never talk to me anymore</td>\n",
       "      <td>counts idk either never talk anymore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>negative</td>\n",
       "      <td>@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.</td>\n",
       "      <td>wouldve first didnt gun really though zac snyders doucheclown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>negative</td>\n",
       "      <td>@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!</td>\n",
       "      <td>wish got watch miss premiere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>negative</td>\n",
       "      <td>Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?</td>\n",
       "      <td>hollis death scene hurt severely watch film wry directors cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>negative</td>\n",
       "      <td>about to file taxes</td>\n",
       "      <td>file taxes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>negative</td>\n",
       "      <td>@LettyA ahh ive always wanted to see rent  love the soundtrack!!</td>\n",
       "      <td>ahh ive always wanted see rent love soundtrack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>negative</td>\n",
       "      <td>@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?</td>\n",
       "      <td>oh dear drinking forgotten table drinks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>negative</td>\n",
       "      <td>@alydesigns i was out most of the day so didn't get much done</td>\n",
       "      <td>day didnt get much done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>negative</td>\n",
       "      <td>one of my friend called me, and asked to meet with her at Mid Valley today...but i've no time *sigh*</td>\n",
       "      <td>one friend called asked meet mid valley todaybut ive time sigh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>negative</td>\n",
       "      <td>@angry_barista I baked you a cake but I ated it</td>\n",
       "      <td>baked cake ated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>negative</td>\n",
       "      <td>this week is not going as i had hoped</td>\n",
       "      <td>week going hoped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>negative</td>\n",
       "      <td>blagh class at 8 tomorrow</td>\n",
       "      <td>blagh class 8 tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>negative</td>\n",
       "      <td>I hate when I have to call and wake people up</td>\n",
       "      <td>hate call wake people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>negative</td>\n",
       "      <td>Just going to cry myself to sleep after watching Marley and Me.</td>\n",
       "      <td>going cry sleep watching marley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>negative</td>\n",
       "      <td>im sad now  Miss.Lilly</td>\n",
       "      <td>im sad misslilly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>negative</td>\n",
       "      <td>ooooh.... LOL  that leslie.... and ok I won't do it again so leslie won't  get mad again</td>\n",
       "      <td>ooooh lol leslie ok wont leslie wont get mad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>negative</td>\n",
       "      <td>Meh... Almost Lover is the exception... this track gets me depressed every time.</td>\n",
       "      <td>meh almost lover exception track gets depressed every time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899970</th>\n",
       "      <td>positive</td>\n",
       "      <td>@fipz i know it is stepford wives. I do live on st mary's island. I quite like that kind of thing!</td>\n",
       "      <td>know stepford wives live st marys island quite like kind thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899971</th>\n",
       "      <td>positive</td>\n",
       "      <td>@tpleeza thank yu</td>\n",
       "      <td>thank yu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899972</th>\n",
       "      <td>positive</td>\n",
       "      <td>I will update my Twitter in the middle of the night to piss everyone off that has me linked to their phone</td>\n",
       "      <td>update twitter middle night piss everyone linked phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899973</th>\n",
       "      <td>positive</td>\n",
       "      <td>Almost done Sunday tasks including open sourcing some code, launching the blog &amp;amp; testing, testing, testing.</td>\n",
       "      <td>almost done sunday tasks including open sourcing code launching blog amp testing testing testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899974</th>\n",
       "      <td>positive</td>\n",
       "      <td>@HLB_LAVISH_MAG Excatly!</td>\n",
       "      <td>excatly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899975</th>\n",
       "      <td>positive</td>\n",
       "      <td>@drellbee when are we going lens shopping? I want a macro now - i will research</td>\n",
       "      <td>going lens shopping want macro research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899976</th>\n",
       "      <td>positive</td>\n",
       "      <td>@ShalG  hai naa..by *also*, do you refer to my other obssessions or imply you are a fan yourself?</td>\n",
       "      <td>hai naaby also refer obssessions imply fan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899977</th>\n",
       "      <td>positive</td>\n",
       "      <td>uploading images to the web... yep Listya &amp;amp; Hendik... not long to go</td>\n",
       "      <td>uploading images web yep listya amp hendik long go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899978</th>\n",
       "      <td>positive</td>\n",
       "      <td>@DumbLittleBunny  newbies.. malakai and thanh li were stirring up trouble last night. loved it. you should read malakai's bi. i loved it.</td>\n",
       "      <td>newbies malakai thanh li stirring trouble last night loved read malakais bi loved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899979</th>\n",
       "      <td>positive</td>\n",
       "      <td>sucking cock tonite</td>\n",
       "      <td>sucking cock tonite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899980</th>\n",
       "      <td>positive</td>\n",
       "      <td>Red Symonds for Prime Minister</td>\n",
       "      <td>red symonds prime minister</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899981</th>\n",
       "      <td>positive</td>\n",
       "      <td>@tishh oh just say it.</td>\n",
       "      <td>oh say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899982</th>\n",
       "      <td>positive</td>\n",
       "      <td>@720perth Lushy, can I tee up an interview for you with the MD of Sexpo for Friday? a nice English chap with a wry sense of humour</td>\n",
       "      <td>lushy tee interview md sexpo friday nice english chap wry sense humour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899983</th>\n",
       "      <td>positive</td>\n",
       "      <td>@JoelMadden So sorry for tweeting to you..I should have read your tweet right the first time and I would have just said Goodnight.</td>\n",
       "      <td>sorry tweeting youi read tweet right first time would said goodnight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899984</th>\n",
       "      <td>positive</td>\n",
       "      <td>my pre 2nd brain surgery song...   ? http://blip.fm/~5jbhh</td>\n",
       "      <td>pre 2nd brain surgery song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899985</th>\n",
       "      <td>positive</td>\n",
       "      <td>@LMarie21 awwwwwww thanks, yeah..real dope flick  more to come</td>\n",
       "      <td>awwwwwww thanks yeahreal dope flick come</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899986</th>\n",
       "      <td>positive</td>\n",
       "      <td>http://twitpic.com/4j8r0 - wanna laugh?!  this is for SUMMER!</td>\n",
       "      <td>wanna laugh summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899987</th>\n",
       "      <td>positive</td>\n",
       "      <td>Video of the arch yesterday - taken by @BrianKerrPhoto - Bail Hill Striding Arch video - I am on this too   http://tinyurl.com/c3q6rd</td>\n",
       "      <td>video arch yesterday taken bail hill striding arch video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899988</th>\n",
       "      <td>positive</td>\n",
       "      <td>Wow, twitter is getting some serious traffic now.... when's Google going to buy them</td>\n",
       "      <td>wow twitter getting serious traffic whens google going buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899989</th>\n",
       "      <td>positive</td>\n",
       "      <td>am i destined to be a e-harmony user??  sometimes i wonder</td>\n",
       "      <td>destined eharmony user sometimes wonder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899990</th>\n",
       "      <td>positive</td>\n",
       "      <td>@Hollywood_Trey how u doinn sexc</td>\n",
       "      <td>u doinn sexc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899991</th>\n",
       "      <td>positive</td>\n",
       "      <td>@amyyyishere why do you have 3 'y's in ur name? anyway, hi! post something. your page is empty  (ok, i'll spell ur name right this time)</td>\n",
       "      <td>3 ys ur name anyway hi post something page empty ok ill spell ur name right time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899992</th>\n",
       "      <td>positive</td>\n",
       "      <td>@yoomyee haha sorry about that LOL. Btw nice to meet u</td>\n",
       "      <td>haha sorry lol btw nice meet u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899993</th>\n",
       "      <td>positive</td>\n",
       "      <td>Ok. On my way to work</td>\n",
       "      <td>ok way work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899994</th>\n",
       "      <td>positive</td>\n",
       "      <td>you have got to be kidding me @ladypn Are you ok ?  ? http://blip.fm/~5jbhr</td>\n",
       "      <td>got kidding ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899995</th>\n",
       "      <td>positive</td>\n",
       "      <td>@jvdouglas  haha, no, the remark on maternity leave fired me up a little</td>\n",
       "      <td>haha remark maternity leave fired little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899996</th>\n",
       "      <td>positive</td>\n",
       "      <td>@altitis and to you!</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899997</th>\n",
       "      <td>positive</td>\n",
       "      <td>Okie doke!! Time for me to escape for the North while Massa's back is turned. Be on when I get home folks</td>\n",
       "      <td>okie doke time escape north massas back turned get home folks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899998</th>\n",
       "      <td>positive</td>\n",
       "      <td>finished the lessons, hooray!</td>\n",
       "      <td>finished lessons hooray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899999</th>\n",
       "      <td>positive</td>\n",
       "      <td>Some ppl are just fucking KP0. Cb ! Stop asking me laa.. I love my boyfriend and thats it.</td>\n",
       "      <td>ppl fucking kp0 cb stop asking laa love boyfriend thats</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment  \\\n",
       "0       negative   \n",
       "1       negative   \n",
       "2       negative   \n",
       "3       negative   \n",
       "4       negative   \n",
       "5       negative   \n",
       "6       negative   \n",
       "7       negative   \n",
       "8       negative   \n",
       "9       negative   \n",
       "10      negative   \n",
       "11      negative   \n",
       "12      negative   \n",
       "13      negative   \n",
       "14      negative   \n",
       "15      negative   \n",
       "16      negative   \n",
       "17      negative   \n",
       "18      negative   \n",
       "19      negative   \n",
       "20      negative   \n",
       "21      negative   \n",
       "22      negative   \n",
       "23      negative   \n",
       "24      negative   \n",
       "25      negative   \n",
       "26      negative   \n",
       "27      negative   \n",
       "28      negative   \n",
       "29      negative   \n",
       "...          ...   \n",
       "899970  positive   \n",
       "899971  positive   \n",
       "899972  positive   \n",
       "899973  positive   \n",
       "899974  positive   \n",
       "899975  positive   \n",
       "899976  positive   \n",
       "899977  positive   \n",
       "899978  positive   \n",
       "899979  positive   \n",
       "899980  positive   \n",
       "899981  positive   \n",
       "899982  positive   \n",
       "899983  positive   \n",
       "899984  positive   \n",
       "899985  positive   \n",
       "899986  positive   \n",
       "899987  positive   \n",
       "899988  positive   \n",
       "899989  positive   \n",
       "899990  positive   \n",
       "899991  positive   \n",
       "899992  positive   \n",
       "899993  positive   \n",
       "899994  positive   \n",
       "899995  positive   \n",
       "899996  positive   \n",
       "899997  positive   \n",
       "899998  positive   \n",
       "899999  positive   \n",
       "\n",
       "                                                                                                                                     text_original  \\\n",
       "0       @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D                          \n",
       "1       is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!                              \n",
       "2       @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                                                    \n",
       "3       my whole body feels itchy and like its on fire                                                                                               \n",
       "4       @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.                               \n",
       "5       @Kwesidei not the whole crew                                                                                                                 \n",
       "6       Need a hug                                                                                                                                   \n",
       "7       @LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?                                          \n",
       "8       @Tatiana_K nope they didn't have it                                                                                                          \n",
       "9       @twittera que me muera ?                                                                                                                     \n",
       "10      spring break in plain city... it's snowing                                                                                                   \n",
       "11      I just re-pierced my ears                                                                                                                    \n",
       "12      @caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .                                               \n",
       "13      @octolinz16 It it counts, idk why I did either. you never talk to me anymore                                                                 \n",
       "14      @smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.                        \n",
       "15      @iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!                                      \n",
       "16      Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?                                                \n",
       "17      about to file taxes                                                                                                                          \n",
       "18      @LettyA ahh ive always wanted to see rent  love the soundtrack!!                                                                             \n",
       "19      @FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?                                                               \n",
       "20      @alydesigns i was out most of the day so didn't get much done                                                                                \n",
       "21      one of my friend called me, and asked to meet with her at Mid Valley today...but i've no time *sigh*                                         \n",
       "22      @angry_barista I baked you a cake but I ated it                                                                                              \n",
       "23      this week is not going as i had hoped                                                                                                        \n",
       "24      blagh class at 8 tomorrow                                                                                                                    \n",
       "25      I hate when I have to call and wake people up                                                                                                \n",
       "26      Just going to cry myself to sleep after watching Marley and Me.                                                                              \n",
       "27      im sad now  Miss.Lilly                                                                                                                       \n",
       "28      ooooh.... LOL  that leslie.... and ok I won't do it again so leslie won't  get mad again                                                     \n",
       "29      Meh... Almost Lover is the exception... this track gets me depressed every time.                                                             \n",
       "...                                                                                   ...                                                            \n",
       "899970  @fipz i know it is stepford wives. I do live on st mary's island. I quite like that kind of thing!                                           \n",
       "899971  @tpleeza thank yu                                                                                                                            \n",
       "899972  I will update my Twitter in the middle of the night to piss everyone off that has me linked to their phone                                   \n",
       "899973  Almost done Sunday tasks including open sourcing some code, launching the blog &amp; testing, testing, testing.                              \n",
       "899974  @HLB_LAVISH_MAG Excatly!                                                                                                                     \n",
       "899975  @drellbee when are we going lens shopping? I want a macro now - i will research                                                              \n",
       "899976  @ShalG  hai naa..by *also*, do you refer to my other obssessions or imply you are a fan yourself?                                            \n",
       "899977  uploading images to the web... yep Listya &amp; Hendik... not long to go                                                                     \n",
       "899978  @DumbLittleBunny  newbies.. malakai and thanh li were stirring up trouble last night. loved it. you should read malakai's bi. i loved it.    \n",
       "899979  sucking cock tonite                                                                                                                          \n",
       "899980  Red Symonds for Prime Minister                                                                                                               \n",
       "899981  @tishh oh just say it.                                                                                                                       \n",
       "899982  @720perth Lushy, can I tee up an interview for you with the MD of Sexpo for Friday? a nice English chap with a wry sense of humour           \n",
       "899983  @JoelMadden So sorry for tweeting to you..I should have read your tweet right the first time and I would have just said Goodnight.           \n",
       "899984  my pre 2nd brain surgery song...   ? http://blip.fm/~5jbhh                                                                                   \n",
       "899985  @LMarie21 awwwwwww thanks, yeah..real dope flick  more to come                                                                               \n",
       "899986  http://twitpic.com/4j8r0 - wanna laugh?!  this is for SUMMER!                                                                                \n",
       "899987  Video of the arch yesterday - taken by @BrianKerrPhoto - Bail Hill Striding Arch video - I am on this too   http://tinyurl.com/c3q6rd        \n",
       "899988  Wow, twitter is getting some serious traffic now.... when's Google going to buy them                                                         \n",
       "899989  am i destined to be a e-harmony user??  sometimes i wonder                                                                                   \n",
       "899990  @Hollywood_Trey how u doinn sexc                                                                                                             \n",
       "899991  @amyyyishere why do you have 3 'y's in ur name? anyway, hi! post something. your page is empty  (ok, i'll spell ur name right this time)     \n",
       "899992  @yoomyee haha sorry about that LOL. Btw nice to meet u                                                                                       \n",
       "899993  Ok. On my way to work                                                                                                                        \n",
       "899994  you have got to be kidding me @ladypn Are you ok ?  ? http://blip.fm/~5jbhr                                                                  \n",
       "899995  @jvdouglas  haha, no, the remark on maternity leave fired me up a little                                                                     \n",
       "899996  @altitis and to you!                                                                                                                         \n",
       "899997  Okie doke!! Time for me to escape for the North while Massa's back is turned. Be on when I get home folks                                    \n",
       "899998  finished the lessons, hooray!                                                                                                                \n",
       "899999  Some ppl are just fucking KP0. Cb ! Stop asking me laa.. I love my boyfriend and thats it.                                                   \n",
       "\n",
       "                                                                                           text_modified  \n",
       "0       awww thats bummer shoulda got david carr third day                                                \n",
       "1       upset cant update facebook texting might cry result school today also blah                        \n",
       "2       dived many times ball managed save 50 rest go bounds                                              \n",
       "3       whole body feels itchy like fire                                                                  \n",
       "4       behaving im mad cant see                                                                          \n",
       "5       whole crew                                                                                        \n",
       "6       need hug                                                                                          \n",
       "7       hey long time see yes rains bit bit lol im fine thanks hows                                       \n",
       "8       nope didnt                                                                                        \n",
       "9       que muera                                                                                         \n",
       "10      spring break plain city snowing                                                                   \n",
       "11      repierced ears                                                                                    \n",
       "12      couldnt bear watch thought ua loss embarrassing                                                   \n",
       "13      counts idk either never talk anymore                                                              \n",
       "14      wouldve first didnt gun really though zac snyders doucheclown                                     \n",
       "15      wish got watch miss premiere                                                                      \n",
       "16      hollis death scene hurt severely watch film wry directors cut                                     \n",
       "17      file taxes                                                                                        \n",
       "18      ahh ive always wanted see rent love soundtrack                                                    \n",
       "19      oh dear drinking forgotten table drinks                                                           \n",
       "20      day didnt get much done                                                                           \n",
       "21      one friend called asked meet mid valley todaybut ive time sigh                                    \n",
       "22      baked cake ated                                                                                   \n",
       "23      week going hoped                                                                                  \n",
       "24      blagh class 8 tomorrow                                                                            \n",
       "25      hate call wake people                                                                             \n",
       "26      going cry sleep watching marley                                                                   \n",
       "27      im sad misslilly                                                                                  \n",
       "28      ooooh lol leslie ok wont leslie wont get mad                                                      \n",
       "29      meh almost lover exception track gets depressed every time                                        \n",
       "...                                                            ...                                        \n",
       "899970  know stepford wives live st marys island quite like kind thing                                    \n",
       "899971  thank yu                                                                                          \n",
       "899972  update twitter middle night piss everyone linked phone                                            \n",
       "899973  almost done sunday tasks including open sourcing code launching blog amp testing testing testing  \n",
       "899974  excatly                                                                                           \n",
       "899975  going lens shopping want macro research                                                           \n",
       "899976  hai naaby also refer obssessions imply fan                                                        \n",
       "899977  uploading images web yep listya amp hendik long go                                                \n",
       "899978  newbies malakai thanh li stirring trouble last night loved read malakais bi loved                 \n",
       "899979  sucking cock tonite                                                                               \n",
       "899980  red symonds prime minister                                                                        \n",
       "899981  oh say                                                                                            \n",
       "899982  lushy tee interview md sexpo friday nice english chap wry sense humour                            \n",
       "899983  sorry tweeting youi read tweet right first time would said goodnight                              \n",
       "899984  pre 2nd brain surgery song                                                                        \n",
       "899985  awwwwwww thanks yeahreal dope flick come                                                          \n",
       "899986  wanna laugh summer                                                                                \n",
       "899987  video arch yesterday taken bail hill striding arch video                                          \n",
       "899988  wow twitter getting serious traffic whens google going buy                                        \n",
       "899989  destined eharmony user sometimes wonder                                                           \n",
       "899990  u doinn sexc                                                                                      \n",
       "899991  3 ys ur name anyway hi post something page empty ok ill spell ur name right time                  \n",
       "899992  haha sorry lol btw nice meet u                                                                    \n",
       "899993  ok way work                                                                                       \n",
       "899994  got kidding ok                                                                                    \n",
       "899995  haha remark maternity leave fired little                                                          \n",
       "899996                                                                                                    \n",
       "899997  okie doke time escape north massas back turned get home folks                                     \n",
       "899998  finished lessons hooray                                                                           \n",
       "899999  ppl fucking kp0 cb stop asking laa love boyfriend thats                                           \n",
       "\n",
       "[200000 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment['text_lemmatized'] = df_sentiment.apply(lambda x: lemmatized(x['text_modified'], WordNetLemmatizer()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FreqDist(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
